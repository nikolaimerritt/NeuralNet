<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Activation | NeuralNet </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Activation | NeuralNet ">
    <meta name="generator" content="docfx 2.58.4.0">
    
    <link rel="shortcut icon" href="../images/icon.png">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/night-owl.min.css">
    <link rel="stylesheet" href="../styles/colors.css">
    <link rel="stylesheet" href="../styles/discord.css">
    <link rel="stylesheet" href="../styles/main.css">
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
    <meta property="docfx:rel" content="../">
    <meta property="docfx:newtab" content="true">
  </head>

  <body>
        <div class="top-navbar">

            <a href="javascript:void(0);" class="burger-icon" onclick="toggleMenu()">
                <svg name="Hamburger" style="vertical-align: middle;" width="24" height="24" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" clip-rule="evenodd" d="M20 6H4V9H20V6ZM4 10.999H20V13.999H4V10.999ZM4 15.999H20V18.999H4V15.999Z"></path></svg>
            </a>

            
            <a class="brand" href="../index.html">
              <img src="../images/icon.png" alt="" class="logomark">
              <span class="brand-title"></span>
            </a>
        </div>

        <div class="body-content">

            <div id="blackout" class="blackout" onclick="toggleMenu()"></div>

            <nav id="sidebar" role="navigation">

                <div class="sidebar">
                    
                    
                    
                    
                    <div>
                      
                      <a class="brand" href="../index.html">
                        <img src="../images/icon.png" alt="" class="logomark">
                        <span class="brand-title"></span>
                      </a>
                      <div id="navbar">
                    
                      </div>
                    
                    </div>


                    <div class="sidebar-item-separator"></div>

                        
                        <div id="sidetoggle">
                          <div id="sidetoc"></div>
                        </div>

                </div>

                <div class="footer">
                  
                  <span>Generated by <strong>DocFX</strong></span>
                </div>
            </nav>

            <main class="main-panel">

                <div role="main" class="hide-when-search">

                        
                        <div class="subnav navbar navbar-default">
                          <div class="container hide-when-search" id="breadcrumb">
                            <ul class="breadcrumb">
                              <li></li>
                            </ul>
                          </div>
                        </div>

                    <article class="content wrap" id="_content" data-uid="activation">
<h1 id="activation">Activation</h1>

<p>The <a class="xref" href="../api/NeuralNetLearning.Maths.Activations.Activation.html">Activation</a> class represents an <a href="https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253">activation function</a> that is applied in <code>NeuralNet</code>.</p>
<h2 id="use-in-initialising-a-neuralnet">Use in initialising a <code>NeuralNet</code></h2>
<p>When initialising a <a class="xref" href="neural-net.html"><code>NeuralNet</code></a> using <a class="xref" href="neural-net-factory.html"><code>NeuralNetFactory</code></a>, an <code>Activation</code> is supplied to each <a class="xref" href="neural-layer-config.html#hidden-layer">Hidden Layer</a> and <a class="xref" href="neural-layer-config.html#output-layer">Output Layer</a>. To see an example, click <a class="xref" href="neural-net-factory.html#layer-structure">here</a>.</p>
<h2 id="provided-activations">Provided <code>Activation</code>s</h2>
<p>There are four <code>Activation</code>s provided by default:</p>
<ul>
<li><a class="xref" href="activation.html#relu-activation">ReLU</a></li>
<li><a class="xref" href="activation.html#sigmoid-activation">Sigmoid</a></li>
<li><a href="xref:activations#tanh-activation">Tanh</a></li>
<li><a href="xref:activations#identity-activation">Identity</a></li>
<li><a href="xref:activations#softmax-activation">Softmax</a></li>
</ul>
<h3 id="relu-activation">ReLU activation</h3>
<p><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU activation</a> is provided through <a class="xref" href="../api/NeuralNetLearning.Maths.Activations.ReluActivation.html"><code>ReluActivation</code></a>. <code>ReluActivation</code>has a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU">leak</a> of zero by default, but can be given a leak value through the optional argument <code>leak</code>.</p>
<p>ReLU activation works well for most projects. If you are unsure of what activation function to use, ReLU activation (with little or no leak) is usually a good option.</p>
<p>The neural layers in initialising a <a class="xref" href="neural-net.html"><code>NeuralNet</code></a> can be set to use <code>ReluActivation</code> by <a class="xref" href="neural-net-factory.html#layer-structure">setting the layer structure</a> to, for example:</p>
<pre><code class="lang-cs">List&lt;NeuralLayerConfig&gt; layerStructure = new ()
{
    ...
    new HiddenLayer(size: 100, activation: new ReluActivation()),
    new HiddenLayer(size: 100, activation: new ReluActivation(leak: 0.01)),
    ...
};
</code></pre>
<h3 id="sigmoid-activation">Sigmoid activation</h3>
<p><a href="https://mathworld.wolfram.com/SigmoidFunction.html">Sigmoid activation</a> (here, the logistic sigmoid) is provided through <a href="xref:NeuralNetLearning.Maths.Activations.SigmoidActivation"><code>SigmoidActivation</code></a>. <code>SigmoidActivation</code> takes no arguments.</p>
<p>The outputs of the sigmoid activation function lie between 0 and 1. This range of values has lead to sigmoid activation often being used in statistics.</p>
<p>The neural layers in initialising a <a class="xref" href="neural-net.html"><code>NeuralNet</code></a> can be set to use <code>SigmoidActivation</code> by <a class="xref" href="neural-net-factory.html#layer-structure">setting the layer structure</a> to, for example:</p>
<pre><code class="lang-cs">List&lt;NeuralLayerConfig&gt; layerStructure = new ()
{
    ...
    new HiddenLayer(size: 100, activation: new SigmoidActivation()),
    ...
};
</code></pre>
<h3 id="tanh-activation">Tanh activation</h3>
<p><a href="https://brenocon.com/blog/2013/10/tanh-is-a-rescaled-logistic-sigmoid-function/">Tanh activation</a> is provided through <a class="xref" href="../api/NeuralNetLearning.Maths.Activations.TanhActivation.html"><code>TanhActivation</code></a>. <code>TanhActivation</code> takes no arguments.</p>
<p>Tanh activation can be thought of as rescaling sigmoid activation so that the output lies between -1 and 1. The fact that tanh activation maps zero to zero gives tanh superior properties in gradient descent compared to sigmoid activation (see <a href="https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function">here</a>).</p>
<p>The neural layers in initialising a <a class="xref" href="neural-net.html"><code>NeuralNet</code></a> can be set to use <code>TanhActivation</code> by <a class="xref" href="neural-net-factory.html#layer-structure">setting the layer structure</a> to, for example:</p>
<pre><code class="lang-cs">List&lt;NeuralLayerConfig&gt; layerStructure = new ()
{
    ...
    new HiddenLayer(size: 100, activation: new TanhActivation()),
    ...
};
</code></pre>
<h3 id="identity-activation">Identity activation</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Identity_function">identity function</a>, also known in machine learning as &quot;linear activation&quot;, is provided through <a class="xref" href="../api/NeuralNetLearning.Maths.Activations.IdentityActivation.html"><code>IdentityActivation</code></a>. The identity function does not process its input: it simply returns whatever input was given to it.</p>
<p>If you do not want to force your output values to lie in a particular range, then you should use <code>IdentityActivation</code>. <code>IdentityActivation</code> is used in <code>OutputLayer</code> by default.</p>
<p>The output layer in initialising a <a class="xref" href="neural-net.html"><code>NeuralNet</code></a> can be set to use <code>IdentityActivation</code> by <a class="xref" href="neural-net-factory.html#layer-structure">setting the layer structure</a> to:</p>
<pre><code class="lang-cs">List&lt;NeuralLayerConfig&gt; layerStructure = new ()
{
    ...
    new OutputLayer(size: 100)
};
</code></pre>
<p>or, more explicitly:</p>
<pre><code class="lang-cs">List&lt;NeuralLayerConfig&gt; layerStructure = new ()
{
    ...
    new OutputLayer(size: 100, activation: new IdentityActivation())
};
</code></pre>
<h2 id="softmax-activation">Softmax activation</h2>
<p><a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax activation</a> is provided through <a href="xref:NeuralNetLearning.Maths.Activations.SotmaxActivation"><code>SoftmaxActivation</code></a>. <code>SoftmaxActivation</code> takes no arguments.</p>
<p>Softmax activation returns a vector with each entry between 0 and 1. Put simply, the <code>i</code>th output entry measures how large the <code>i</code>th input entry is compared to all the other input entries, as a ratio between 0 and 1. More precisely:</p>
<ul>
<li>the greater the input entry, the closer the output entry is to 1</li>
<li>all the output entries add to 1</li>
</ul>
<p>This makes softmax activation useful to apply to the output layer if the output vector should be a vector of ratios or a vector of probabilities.</p>
<p>The output layer in initialising a <a class="xref" href="neural-net.html"><code>NeuralNet</code></a> using <a class="xref" href="neural-net-factory.html"><code>NeuralNetFactory</code></a> can be set to use <code>SoftmaxActivation</code> by by <a class="xref" href="neural-net-factory.html#layer-structure">setting the layer structure</a> to:</p>
<pre><code class="lang-cs">List&lt;NeuralLayerConfig&gt; layerStructure = new ()
{
    ...
    new OutputLayer(size: 100, activation: new SoftmaxActivation())
};
</code></pre>
</article>
              
                </div>
            </main>
        </div>

        
<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<script type="text/javascript" src="../styles/jquery.twbsPagination.js"></script>
<script type="text/javascript" src="../styles/url.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
<script type="text/javascript" src="../styles/docfx.js"></script>
<script type="text/javascript" src="../styles/main.js"></script>

    </body>

</html>
