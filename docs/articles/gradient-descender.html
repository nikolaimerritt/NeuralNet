<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Gradient Descender | NeuralNet </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Gradient Descender | NeuralNet ">
    <meta name="generator" content="docfx 2.58.4.0">
    
    <link rel="shortcut icon" href="../images/icon.png">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/night-owl.min.css">
    <link rel="stylesheet" href="../styles/colors.css">
    <link rel="stylesheet" href="../styles/discord.css">
    <link rel="stylesheet" href="../styles/main.css">
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
    <meta property="docfx:rel" content="../">
    <meta property="docfx:newtab" content="true">
  </head>

  <body>
        <div class="top-navbar">

            <a href="javascript:void(0);" class="burger-icon" onclick="toggleMenu()">
                <svg name="Hamburger" style="vertical-align: middle;" width="24" height="24" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" clip-rule="evenodd" d="M20 6H4V9H20V6ZM4 10.999H20V13.999H4V10.999ZM4 15.999H20V18.999H4V15.999Z"></path></svg>
            </a>

            
            <a class="brand" href="../index.html">
              <img src="../images/icon.png" alt="" class="logomark">
              <span class="brand-title"></span>
            </a>
        </div>

        <div class="body-content">

            <div id="blackout" class="blackout" onclick="toggleMenu()"></div>

            <nav id="sidebar" role="navigation">

                <div class="sidebar">
                    
                    
                    
                    
                    <div>
                      
                      <a class="brand" href="../index.html">
                        <img src="../images/icon.png" alt="" class="logomark">
                        <span class="brand-title"></span>
                      </a>
                      <div id="navbar">
                    
                      </div>
                    
                    </div>


                    <div class="sidebar-item-separator"></div>

                        
                        <div id="sidetoggle">
                          <div id="sidetoc"></div>
                        </div>

                </div>

                <div class="footer">
                  
                  <span>Generated by <strong>DocFX</strong></span>
                </div>
            </nav>

            <main class="main-panel">

                <div role="main" class="hide-when-search">

                        
                        <div class="subnav navbar navbar-default">
                          <div class="container hide-when-search" id="breadcrumb">
                            <ul class="breadcrumb">
                              <li></li>
                            </ul>
                          </div>
                        </div>

                    <article class="content wrap" id="_content" data-uid="graddesc">
<h1 id="gradient-descender">Gradient Descender</h1>

<p>The <a class="xref" href="../api/NeuralNetLearning.Maths.GradientDescent.GradientDescender.html">GradientDescender</a> class represents a gradient descent algorithm.</p>
<h2 id="use-in-initialising-a-neuralnet">Use in initialising a <code>NeuralNet</code></h2>
<p>When initialising a <a class="xref" href="neural-net.html">NeuralNet</a>, the gradient descender is passed to <a class="xref" href="neural-net-factory.html#creating-your-neural-network">NeuralNetFactory</a>.</p>
<h2 id="provided-gradient-descenders">Provided Gradient Descenders</h2>
<p>There are two gradient descenders provided by default: Adam and Stochastic gradient descent.</p>
<h3 id="adam-gradient-descender">Adam Gradient Descender</h3>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html#adam">Adam gradient descent</a> is the go-to gradient descent algorithm for most projects. It is a complex algorithm that uses a combination of many different learning rates, which each speed up or slow down independently while learning.</p>
<p>To create an <code>AdamGradientDescender</code> with default settings, simply run</p>
<pre><code class="lang-cs">AdamGradientDescender gradientDescender = new();
</code></pre>
<p>This uses the settings reccommended by the <a href="https://arxiv.org/pdf/1412.6980.pdf">creators</a> of the algorithm. These settings work well for most projects. If you would like to set your own values, however, specify in any of the three hyper-parameters like so:</p>
<pre><code class="lang-cs">AdamGradientDescender gradientDescender = new(learningRate: ..., momentumDecay: ..., varianceDecay: ...);
</code></pre>
<p>With respect to the <a href="https://arxiv.org/pdf/1412.6980.pdf">original article</a>:
- <code>learningRate</code> corresponds to α
- <code>momentumDecay</code> corresponds to β1
- <code>varianceDecay</code> corresponds to β2</p>
<h3 id="stochastic-gradient-descender">Stochastic Gradient Descender</h3>
<p>Stochastic gradient descent (also known as <a href="https://ruder.io/optimizing-gradient-descent/index.html#minibatchgradientdescent">vanilla mini-batch gradient descent</a>) is the simplest gradient descent algorithm. Adam gradient descent may be faster than stochastic gradient descent in most cases, but stochastic gradient descent is a safe fall-back if you suspect adam gradient descent is not suited for your project.</p>
<p>For example, if you think a learning rate of 0.001 is best for your project, you can create a <code>StochasticGradientDescender</code> by</p>
<pre><code class="lang-cs">StochasticGradientDescender gradientDescender = new(learningRate: 0.001);
</code></pre>
<p>Stochastic gradient descent is far simpler than Adam gradient descent. Instead of dynamically speeding up or slowing down learning rates, stochastic gradient descent sticks with one single learning rate throughout the learning process. This means that picking the right learning rate is crucial, and will depend highly on your own project.</p>
<h2 id="making-your-own-gradient-descender-technical">Making your own Gradient Descender (Technical)</h2>
<p>If you have a gradient descent algorithm in mind, you can make your own gradient descender class.</p>
<p>To work with <a class="xref" href="neural-net.html">NeuralNet</a>, and in particular, serialization, your class will have to:
- inherit from the abstract class <code>GradientDescender</code>
- implement all the hyper-parameters you need as fields
- annotate all the hyper-parameters you need with <a class="xref" href="serializable-hyper-parameter.html">SerializableHyperParameter</a>
- have a default constructor
- implement <code>Parameter GradientDescentStep(Parameter gradient)</code></p>
<p>You will need to have read <a class="xref" href="parameter.html">Parameter</a> and <a class="xref" href="serializable-hyper-parameter.html">SerializableHyperParameter</a> before reading this.</p>
<p>Let's implement <a href="https://ruder.io/optimizing-gradient-descent/index.html#momentum">momentum gradient descent</a> as an example:</p>
<p>First, we import the required namespaces for <a class="xref" href="parameter.html">Parameter</a>, <code>GradientDescender</code> and <a class="xref" href="serializable-hyper-parameter.html">SerializableHyperParameter</a> respectively.</p>
<pre><code class="lang-cs">using NeuralNetLearning.Maths;
using NeuralNetLearning.Maths.GradientDescent;
using NeuralNetLearning.Serialization;

public class MomentumGradientDescender : GradientDescender
{
</code></pre>
<p>Next, we mark all the hyper-parameters that we want to be able to save and read in with <a class="xref" href="serializable-hyper-parameter.html">SerializableHyperParameter</a>. These hyper-parameters should be enough to fully reconstruct our gradient descender.</p>
<p>In this example, <code>_learningRate</code> corresponds to η in the <a href="https://ruder.io/optimizing-gradient-descent/index.html#momentum">momentum gradient descent</a> description, and <code>_momentumRate</code> corresponds to γ.</p>
<pre><code class="lang-cs">    [SerializableHyperParameter(&quot;learning rate&quot;)]
    private readonly double _learningRate;

    [SerializableHyperParameter(&quot;momentum rate&quot;)]
    private readonly double _momentumRate;

    [SerializableHyperParameter(&quot;past step&quot;)]
    private Parameter _pastStep = null;
</code></pre>
<p>We then create our constructor for these fields. A default constructor is also required for the automatic serialization.</p>
<pre><code class="lang-cs">
    public MomentumGradientDescender(double learningRate, double momentumRate)
    {
        _learningRate = learningRate;
        _momentumRate = momentumRate;
    }

    private MomentumGradientDescender()
        : this(learningRate: 0.01, momentumRate: 0.9)
    { }
</code></pre>
<p>Finally, we implement the momentum gradient descent algorithm in <code>Parameter GradientDescentStep(Parameter gradient)</code>. The function takes in the cost gradient of the weights and biases as a <a class="xref" href="parameter.html">Parameter</a>. This <a class="xref" href="parameter.html">Parameter</a> holds all the cost gradients of the corresponding weight/bias entries.</p>
<p><code>GradientDescentStep(...)</code> returns the step that should be made in parameter space to reduce cost. In other words, the return value will be added to the <a class="xref" href="parameter.html">Parameter</a> holding the weights and biases of the <a class="xref" href="neural-net.html">NeuralNet</a> to reduce cost.</p>
<pre><code class="lang-cs">
    public override Parameter GradientDescentStep(Parameter gradient)
    {
        if (_pastStep == null)
            _pastStep = ParameterFactory.Zero(gradient.LayerSizes);

        Parameter step = -_learningRate * gradient + _momentumRate * _pastStep;
        _pastStep = step;
        return step; // in the NeuralNet class: parameter += step;
    }
}
</code></pre>
</article>
              
                </div>
            </main>
        </div>

        
<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<script type="text/javascript" src="../styles/jquery.twbsPagination.js"></script>
<script type="text/javascript" src="../styles/url.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
<script type="text/javascript" src="../styles/docfx.js"></script>
<script type="text/javascript" src="../styles/main.js"></script>

    </body>

</html>
